# -*- coding: utf-8 -*-
"""TF-IDF.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1_DDFZC0-gwAXvm2FJN3Z-el4_1LanxeH

### **با کتابخانه Scikit-learn**
"""

!pip install -U scikit-learn

from sklearn.feature_extraction.text import TfidfVectorizer

# لیستی از اسناد (هر سند یک جمله یا پاراگراف)
documents = [
    "هوش مصنوعی آینده را تغییر خواهد داد",
    "یادگیری ماشین یکی از شاخه‌های هوش مصنوعی است",
    "هوش مصنوعی در پزشکی و آموزش کاربرد دارد"
]

# ساخت TF-IDF مدل
vectorizer = TfidfVectorizer()
X = vectorizer.fit_transform(documents)

# نمایش کلمات و وزن TF-IDF
words = vectorizer.get_feature_names_out()

for doc_index, doc_vector in enumerate(X.toarray()):
    print(f"\n🔹 سند {doc_index + 1}:")
    for word_index, score in enumerate(doc_vector):
        if score > 0:
            print(f"{words[word_index]}: {score:.4f}")

"""### **بدون کتابخانه**"""

import math
from collections import Counter

# سندها
documents = [
    "هوش مصنوعی آینده را تغییر خواهد داد",
    "یادگیری ماشین یکی از شاخه‌های هوش مصنوعی است",
    "هوش مصنوعی در پزشکی و آموزش کاربرد دارد"
]

# پیش‌پردازش: جدا کردن کلمات
tokenized_docs = [doc.split() for doc in documents]

# تعداد کل سندها
N = len(tokenized_docs)

# محاسبه DF: برای هر کلمه در چند سند ظاهر شده؟
df = {}
for doc in tokenized_docs:
    unique_words = set(doc)
    for word in unique_words:
        df[word] = df.get(word, 0) + 1

# محاسبه TF-IDF
for doc_index, doc in enumerate(tokenized_docs):
    print(f"\n🔹 سند {doc_index + 1}:")
    word_counts = Counter(doc)
    total_words = len(doc)

    for word in word_counts:
        tf = word_counts[word] / total_words
        idf = math.log(N / (1 + df[word]))
        tfidf = tf * idf
        print(f"{word}: {tfidf:.4f}")